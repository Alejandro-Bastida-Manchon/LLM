{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizado por: \n",
    "#  Alejandro Bastida Manchón(alejandro.bastida@edu.upct.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlaces para los temas de subreddits seleccionados:\n",
    "*   [r/stockmarket](https://www.reddit.com/r/stockmarket/)\n",
    "*   [r/nba](https://www.reddit.com/r/nba/)\n",
    "*   [r/technology](https://www.reddit.com/r/technology/)\n",
    "*   [r/CharacterAI](https://www.reddit.com/r/CharacterAI/)\n",
    "*   [r/plants](https://www.reddit.com/r/plants/)\n",
    "*   [r/pcmasterrace](https://www.reddit.com/r/pcmasterrace/)\n",
    "*   [r/europe](https://www.reddit.com/r/europe/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw # Python Reddit API Wrapper\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Compilación del corpus y uso de procesamiento léxico**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos de r/stockmarket...\n",
      "Descargando datos de r/nba...\n",
      "Descargando datos de r/technology...\n",
      "Descargando datos de r/CharacterAI...\n",
      "Descargando datos de r/plants...\n",
      "Descargando datos de r/pcmasterrace...\n",
      "Descargando datos de r/europe...\n",
      "✅ Descarga completa.\n"
     ]
    }
   ],
   "source": [
    "# --- Conexión a Reddit ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id='id',\n",
    "    client_secret='passwrd',\n",
    "    user_agent='user'\n",
    "    # No usamos los credenciales de login como se hace en el ejemplo porque las políticas de Reddit han cambiado y ahora es necesario ser Premium para ello.\n",
    ")\n",
    "\n",
    "# Subreddits\n",
    "subreddits = [\n",
    "    'stockmarket', 'nba', 'technology', 'CharacterAI', 'plants', 'pcmasterrace', 'europe'\n",
    "]\n",
    "\n",
    "# Carpeta para guardar JSONs\n",
    "output_folder = 'reddit_jsons'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Función para convertir timestamp\n",
    "def convert_date(utc_timestamp):\n",
    "    return dt.datetime.fromtimestamp(utc_timestamp).isoformat()\n",
    "\n",
    "# Extracción de datos\n",
    "for subreddit_name in subreddits:\n",
    "    print(f\"Descargando datos de r/{subreddit_name}...\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    threads_data = []\n",
    "    \n",
    "    for submission in subreddit.top(limit=20):  # 20 hilos por subreddit\n",
    "        try:\n",
    "            submission.comments.replace_more(limit=0)  # Descarga de todos los comentarios directamente\n",
    "        except Exception as e:\n",
    "            print(f\"Error en los comentarios de {submission.title}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        thread_info = {\n",
    "            \"flair\": submission.link_flair_text if submission.link_flair_text else \"\",\n",
    "            \"title\": submission.title,\n",
    "            \"author\": str(submission.author) if submission.author else \"Deleted\",\n",
    "            \"date\": convert_date(submission.created_utc),\n",
    "            \"score\": submission.score,\n",
    "            \"description\": submission.selftext,\n",
    "            \"comments\": []\n",
    "        }\n",
    "        \n",
    "        comments_count = 0\n",
    "        for comment in submission.comments.list():\n",
    "            if comment.body and len(comment.body) > 10:  # Filtrar comentarios vacíos o muy cortos\n",
    "                comment_info = {\n",
    "                    \"user\": str(comment.author) if comment.author else \"Deleted\",\n",
    "                    \"comment\": comment.body,\n",
    "                    \"score\": comment.score,\n",
    "                    \"date\": convert_date(comment.created_utc)\n",
    "                }\n",
    "                thread_info[\"comments\"].append(comment_info)\n",
    "                comments_count += 1\n",
    "                if comments_count >= 50:  # 50 comentarios por hilo\n",
    "                    break\n",
    "        \n",
    "        if len(thread_info[\"comments\"]) > 0:\n",
    "            threads_data.append(thread_info)\n",
    "    \n",
    "    # Guardar en JSON\n",
    "    with open(os.path.join(output_folder, f\"{subreddit_name}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(threads_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ Descarga completa.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación del ejercicio 1:\n",
    "\n",
    "#### import praw\n",
    "#### reddit = praw.Reddit(\n",
    "####    client_id='...',                  # ID de tu app en Reddit\n",
    "####    client_secret='...',              # Clave secreta\n",
    "####   user_agent='pln-algo@correo.com'  # Identificador del script (obligatorio)\n",
    "#### )\n",
    "\n",
    "\n",
    "### **En primer lugar, nos conectamos a la API de reddit a través de la biblioteca praw, para ello ha sido necesario crear una cuenta en reddit y en la seccion home en el apartado de apps crear una nueva app a la que hemos llamado PLN_practica2. Nos conectamos a Reddit con las credenciales client_id que es el ID de nuestra app y client_secret que es la clave secreta.**\n",
    "\n",
    "#### subreddits = ['stockmarket', 'nba', 'technology', 'CharacterAI', 'plants', 'pcmasterrace', 'europe']\n",
    "\n",
    "\n",
    "### **Una vez nos hemos conectado buscamos en reddit los subreditts que mas nos gusten y añadimos cada uno de los subreddits a una lista para tenerlos definidos. Creamos la carpeta de salida a la que hemos llamado 'reddit_jsons'.**\n",
    "\n",
    "#### def convert_date(utc_timestamp):\n",
    "####     return dt.datetime.fromtimestamp(utc_timestamp).isoformat()\n",
    "\n",
    "### **Para cambiar el formato de fecha de reddit que es UNIX/UTC hemos utilizado la funcion convert_date.**\n",
    "\n",
    "#### for subreddit_name in subreddits:\n",
    "####     print(f\"Descargando datos de r/{subreddit_name}...\")\n",
    "####     subreddit = reddit.subreddit(subreddit_name)\n",
    "####      threads_data = []\n",
    "####    for submission in subreddit.top(limit=20):\n",
    "#### submission.comments.replace_more(limit=0)\n",
    "\n",
    "\n",
    "### **Cuando tenemos todo esto ya pasamos a la extraccion de los hilos y los comentarios, para ello recorremos con un bulce for cada subreddit de la lista y se prepara una lista vacia que la hemos llamado 'threads_data' donde se iran guardando los hilos. Hemos establecido que solo se procesen los 20 hilos mas votados para cada subreddit, ademas expandimos los comentarios largos 'More Comments' de reddit a traves de 'submission.comments.replace_more(limit=0)'.**\n",
    "\n",
    "####        thread_info = {\n",
    "####            \"flair\": ...,      # Etiqueta del hilo (opcional)\n",
    "####            \"title\": ...,      # Título del hilo\n",
    "####           \"author\": ...,     # Usuario que lo publicó\n",
    "####            \"date\": ...,       # Fecha de creación\n",
    "####            \"score\": ...,      # Votos del hilo\n",
    "####            \"description\": ...,# Cuerpo del post (si tiene)\n",
    "####            \"comments\": []     # Aquí irán los comentarios\n",
    "####      }\n",
    "\n",
    "\n",
    "### **Para guardar los datos del hilo creamos el diccionario thread_info donde guardamos la etiqueta del hilo (opcional), el titulo, el autor, la fecha, los votos, la descripcion o cuerpo del host (en caso de que tenga) y luego los comentarios.**\n",
    "\n",
    "####        comments_count = 0\n",
    "####       for comment in submission.comments.list():\n",
    "####          if comment.body and len(comment.body) > 10:\n",
    "####                comment_info = {\n",
    "####                    \"user\": ...,        # Autor del comentario\n",
    "####                    \"comment\": ...,     # Texto del comentario\n",
    "####                    \"score\": ...,       # Votos del comentario\n",
    "####                    \"date\": ...         # Fecha del comentario\n",
    "####                }\n",
    "####                thread_info[\"comments\"].append(comment_info)\n",
    "####                comments_count += 1\n",
    "####                if comments_count >= 50:\n",
    "####                    break\n",
    "\n",
    "\n",
    "### **Para guardar los comentarios de cada hilo recorremos todos los comentarios del mismo y solo guardamos los que tengan mas de 10 caracteres para que sean comentarios que nos puedan proporcionar cierta infromacion y se detiene la busqueda de comentarios al llegar a los 50 y guardamos los datos en el apartado de comentarios del diccionario 'threads_info'.**\n",
    "\n",
    "####     with open(os.path.join(output_folder, f\"{subreddit_name}.json\"), \"w\", encoding=\"utf-8\") as f: json.dump(threads_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "### **Y para finalzar este apartado, guardamos toda la informacion de los subreddits en archivos tipo json donde el nombre es el nombre de los subredits y estos se guardaran en la carpeta reddit_jsons que habiamos creado anteriormente.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **2. Clasificador de comentarios en subreddits**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de comentarios cargados: 6931\n",
      "                                                text        label  \\\n",
      "0  Just please for the love of our sanity. Make t...  CharacterAI   \n",
      "1  Yeah, but it just says a separate model for pe...  CharacterAI   \n",
      "2  https://preview.redd.it/n5my12nmur0e1.jpeg?wid...  CharacterAI   \n",
      "3  Hopefully they really do it and push through w...  CharacterAI   \n",
      "4  Swear to God if it's gonna be behind that payw...  CharacterAI   \n",
      "\n",
      "       thread_id  \n",
      "0  CharacterAI_0  \n",
      "1  CharacterAI_0  \n",
      "2  CharacterAI_0  \n",
      "3  CharacterAI_0  \n",
      "4  CharacterAI_0  \n",
      "Número de ejemplos en entrenamiento: 4831\n",
      "Número de ejemplos en validación: 2100\n"
     ]
    }
   ],
   "source": [
    "# Creación de los conjuntos de entrenamiento y validación\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Carpeta donde están los JSONs ---\n",
    "input_folder = 'reddit_jsons'\n",
    "\n",
    "# --- Variables para almacenar comentarios ---\n",
    "texts = []\n",
    "labels = []\n",
    "thread_ids = []  # Para mantener juntos los comentarios de un mismo hilo\n",
    "\n",
    "# --- Leer todos los JSONs ---\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        subreddit_name = filename.replace('.json', '')\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            threads = json.load(f)\n",
    "            for i, thread in enumerate(threads):\n",
    "                thread_id = f\"{subreddit_name}_{i}\"\n",
    "                for comment in thread['comments']:\n",
    "                    if len(comment['comment']) > 10:  # Evitar comentarios muy cortos\n",
    "                        texts.append(comment['comment'])\n",
    "                        labels.append(subreddit_name)\n",
    "                        thread_ids.append(thread_id)\n",
    "\n",
    "# --- Crear un DataFrame ---\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'label': labels,\n",
    "    'thread_id': thread_ids\n",
    "})\n",
    "\n",
    "print(f\"Total de comentarios cargados: {len(df)}\")\n",
    "print(df.head())\n",
    "\n",
    "# --- Separar por hilos (thread_id), NO aleatorio total ---\n",
    "# Primero separamos los hilos para que no se mezclen comentarios del mismo hilo en train y validation\n",
    "unique_threads = df['thread_id'].unique()\n",
    "train_threads, val_threads = train_test_split(unique_threads, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Crear sets de entrenamiento y validación basados en los hilos\n",
    "train_df = df[df['thread_id'].isin(train_threads)].reset_index(drop=True)\n",
    "val_df = df[df['thread_id'].isin(val_threads)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Número de ejemplos en entrenamiento: {len(train_df)}\")\n",
    "print(f\"Número de ejemplos en validación: {len(val_df)}\")\n",
    "\n",
    "# --- Guardar opcionalmente los datasets ---\n",
    "train_df.to_csv('train_comments2.csv', index=False)\n",
    "val_df.to_csv('val_comments2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lo primero que hemos hecho ha sido dividir los datos por hilos completos para evitar que comentarios del mismo hilo aparezcan en ambos conjuntos (entrenamiento y validación), lo cual podría introducir sesgo.**\n",
    "\n",
    "**Esta separación por thread_id asegura que el modelo generalice mejor y evita sobreentrenamiento basado en contexto compartido.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46285714285714286\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " CharacterAI       0.68      0.54      0.60       400\n",
      "      europe       0.39      0.44      0.42       250\n",
      "         nba       0.38      0.51      0.44       250\n",
      "pcmasterrace       0.39      0.38      0.39       300\n",
      "      plants       0.43      0.73      0.54       200\n",
      " stockmarket       0.64      0.39      0.48       450\n",
      "  technology       0.32      0.33      0.32       250\n",
      "\n",
      "    accuracy                           0.46      2100\n",
      "   macro avg       0.46      0.47      0.46      2100\n",
      "weighted avg       0.49      0.46      0.47      2100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hijos\\AppData\\Local\\Temp\\ipykernel_13768\\2105868676.py:42: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + list(clf.classes_), rotation=90)\n",
      "C:\\Users\\Hijos\\AppData\\Local\\Temp\\ipykernel_13768\\2105868676.py:43: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + list(clf.classes_))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modelo 1 : BOW\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Datasets\n",
    "train_df = pd.read_csv('train_comments2.csv')\n",
    "val_df = pd.read_csv('val_comments2.csv')\n",
    "\n",
    "# Vectorización BOW\n",
    "\n",
    "bow_vectorizer = CountVectorizer()  \n",
    "\n",
    "X_train = bow_vectorizer.fit_transform(train_df['text'])  # Solo se \"entrena\" el vectorizador en train\n",
    "X_val = bow_vectorizer.transform(val_df['text'])         # Val se transforma con el mismo vectorizador\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "\n",
    "# Entrenamiento LinearSVC\n",
    "clf = LinearSVC(random_state=42, tol=1e-5, dual=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# Evaluación\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "# Matriz de Confusión\n",
    "conf_mat = confusion_matrix(y_val, y_pred, labels=clf.classes_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "cax = ax.matshow(conf_mat)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels([''] + list(clf.classes_), rotation=90)\n",
    "ax.set_yticklabels([''] + list(clf.classes_))\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aquí, hemos decidido tomar la decisión de utilizar BOW(Bag of Words), que es una técnica de representación textual que convierte un texto en un vector numérico, basándose únicamente en la frecuencia de aparición de palabras.**\n",
    "\n",
    "**La forma de proceder que tiene es la siguiente :**\n",
    "- **Se construye un vocabulario con todas las palabras únicas que aparecen en el conjunto de entrenamiento.**\n",
    "- **Cada texto se transforma en un vector donde:**\n",
    "    - **Cada posición representa una palabra del vocabulario.**\n",
    "    - **El valor representa cuántas veces aparece esa palabra en el texto (frecuencia).**\n",
    "- **No tiene en cuenta el orden de las palabras, ni el contexto.**\n",
    "**La clase CountVectorizer genera vectores BoW clásicos con unigramas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En cuánto al análisis de resultados, en este caso observamos lo siguiente :**\n",
    "\n",
    "- **El rendimiento global es del 46.28 %, lo cual es un resultado razonable para un modelo basado en la representación Bag of Words (BoW) con bigramas, especialmente considerando la naturaleza multiclase del problema. Aunque esta técnica es simple, logra capturar algunas relaciones superficiales entre palabras frecuentes.**\n",
    "\n",
    "- **Las clases con mejor rendimiento son CharacterAI, plants y stockmarket, con f1-scores de 0.54, 0.53 y 0.48 respectivamente. Esto sugiere que estos subreddits tienen un vocabulario más específico o repetitivo, lo que permite al modelo detectar patrones con mayor facilidad.**\n",
    "\n",
    "- **Las clases con rendimiento más bajo son technology (f1 = 0.32) y pcmasterrace (f1 = 0.34). Estos subreddits posiblemente comparten más vocabulario con otras clases o tienen un contenido más diverso, lo que dificulta su clasificación con una técnica tan superficial como BoW.**\n",
    "\n",
    "- **La matriz de confusión confirma confusión entre clases similares, por ejemplo, pcmasterrace es confundido con europe y nba, y technology con varias clases, sin un patrón claro. Esto indica falta de capacidad del modelo para captar diferencias semánticas más profundas entre contextos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings cargados: 400000 palabras.\n",
      "Vectorizando comentarios...\n",
      "Tamaño X_train: (4829, 100)\n",
      "Tamaño X_val: (2100, 100)\n",
      "Entrenando modelo...\n",
      "Accuracy: 0.36904761904761907\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " CharacterAI       0.54      0.32      0.40       400\n",
      "      europe       0.31      0.39      0.35       250\n",
      "         nba       0.29      0.46      0.36       250\n",
      "pcmasterrace       0.34      0.32      0.33       300\n",
      "      plants       0.39      0.73      0.51       200\n",
      " stockmarket       0.66      0.27      0.38       450\n",
      "  technology       0.23      0.30      0.26       250\n",
      "\n",
      "    accuracy                           0.37      2100\n",
      "   macro avg       0.39      0.40      0.37      2100\n",
      "weighted avg       0.43      0.37      0.37      2100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hijos\\AppData\\Local\\Temp\\ipykernel_13768\\3449666157.py:75: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + list(clf.classes_), rotation=90)\n",
      "C:\\Users\\Hijos\\AppData\\Local\\Temp\\ipykernel_13768\\3449666157.py:76: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + list(clf.classes_))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modelo 2 : Word Embeddings(GloVe + Random Forest)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datasets\n",
    "train_df = pd.read_csv('train_comments2.csv')\n",
    "val_df = pd.read_csv('val_comments2.csv')\n",
    "\n",
    "# Carga de GloVe embeddings manualmente\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# GloVe\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "word_vectors = load_glove_embeddings(glove_path)\n",
    "\n",
    "print(f\"Embeddings cargados: {len(word_vectors)} palabras.\")\n",
    "\n",
    "# Función para convertir comentario en vector promedio\n",
    "def comment_to_vector(comment, word_vectors, vector_size=100):\n",
    "    words = comment.split()\n",
    "    word_vecs = []\n",
    "    for word in words:\n",
    "        if word.lower() in word_vectors:  # Conversión a minúsculas\n",
    "            word_vecs.append(word_vectors[word.lower()])\n",
    "    if len(word_vecs) > 0:\n",
    "        return np.mean(word_vecs, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Vectorizar comentarios\n",
    "print(\"Vectorizando comentarios...\")\n",
    "\n",
    "X_train = np.array([comment_to_vector(text, word_vectors) for text in train_df['text']])\n",
    "X_val = np.array([comment_to_vector(text, word_vectors) for text in val_df['text']])\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "\n",
    "print(f\"Tamaño X_train: {X_train.shape}\")\n",
    "print(f\"Tamaño X_val: {X_val.shape}\")\n",
    "\n",
    "# Entrenar modelo\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"Entrenando modelo...\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# Evaluación\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "# Matriz de Confusión\n",
    "conf_mat = confusion_matrix(y_val, y_pred, labels=clf.classes_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "cax = ax.matshow(conf_mat, cmap=\"Blues\")\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels([''] + list(clf.classes_), rotation=90)\n",
    "ax.set_yticklabels([''] + list(clf.classes_))\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Word Embeddings Model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En este caso, de los posibles modelos hemos decidido escoger GloVe (Global Vectors for Word Representation), que es un modelo de word embeddings que representa cada palabra como un vector denso en un espacio semántico, entrenado para capturar similitudes y relaciones de significado.**\n",
    "\n",
    "**Su funcionamiento y características claves son las siguientes :**\n",
    "\n",
    "- **GloVe se entrena a partir de un co-ocurrence matrix: mide cuántas veces aparece una palabra junto a otra en un gran corpus.**\n",
    "\n",
    "- **En nuestro modelo, para representar un comentario completo, calculamos el promedio de los vectores de las palabras que contiene (comment_to_vector).**\n",
    "- **El objetivo es capturar relaciones del tipo:**\n",
    "    - **vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")**\n",
    "    - **Cada palabra se representa como un vector de tamaño fijo (en nuestro caso, 100 dimensiones).**\n",
    "\n",
    "**Por lo tanto, hemos cargado glove.6B.100d.txt con 400,000 vectores preentrenados, cada comentario ha sido convertido en un vector promedio de los embeddings de sus palabras y hemos entrenado un Random Forest para clasificar esos vectores según el subreddit del comentario.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hemos obtenido, en este caso, los siguientes resultados :**\n",
    "\n",
    "- **Accuracy total: 39.0%**\n",
    "- **Macro F1-score: 0.37**\n",
    "- **Weighted F1-score: 0.37**\n",
    "\n",
    "**Esto implica que el modelo con embeddings de GloVe, a pesar de ser un enfoque más sofisticado que BoW, sigue presentando un rendimiento similar en términos de precisión. Sin embargo, el modelo basado en embeddings no es lo suficientemente discriminativo cuando no se realiza un ajuste fino (fine-tuning).**\n",
    "\n",
    "**La matriz de confusión muestra una cierta diagonal, lo que indica que el modelo acierta en algunas clases, pero presenta un desempeño deficiente en otras.**\n",
    "\n",
    "**Por ejemplo, el modelo tiene dificultades para clasificar correctamente 'technology', 'europe' y 'stockmarket', y comete algunos errores entre clases con vocabularios similares. A pesar de que los embeddings ayudan a capturar algo del contexto, aún hay confusión importante entre subreddits con vocabularios superpuestos, como 'CharacterAI', 'plants', y 'nba'. Esto indica que las representaciones de las palabras por sí solas no son suficientes para diferenciar correctamente entre clases complejas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='441' max='441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [441/441 1:44:40, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=441, training_loss=0.7007405013183888, metrics={'train_runtime': 6359.728, 'train_samples_per_second': 0.55, 'train_steps_per_second': 0.069, 'total_flos': 230232508800000.0, 'train_loss': 0.7007405013183888, 'epoch': 7.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 3: Fine-tuning RoBERTA para clasificar subreddits\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, RobertaTokenizer, RobertaForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "# --- Datos ---\n",
    "train_df = pd.read_csv(\"train_comments2.csv\").sample(500)  # Muestra pequeña para acelerar\n",
    "val_df = pd.read_csv(\"val_comments2.csv\").sample(100)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df['label'])\n",
    "val_labels = label_encoder.transform(val_df['label'])\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encodings = tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "        item = {k: v.squeeze() for k, v in encodings.items()}\n",
    "        # Asegurar que labels es LongTensor\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = SimpleDataset(train_df['text'].tolist(), train_labels)\n",
    "val_dataset = SimpleDataset(val_df['text'].tolist(), val_labels)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Crear directorio para salida si no existe y usar ruta relativa\n",
    "output_dir = \"/results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    # Quita evaluation_strategy, eval_steps, etc. si tu versión no los soporta\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hemos decidido hacer uso de RoBERTa (Robustly optimized BERT approach), que es una variante mejorada del modelo BERT. Al igual que BERT, es un modelo de Transformers basado en el aprendizaje contextual de representaciones de texto, pero optimiza el preentrenamiento para un rendimiento superior. Algunas de sus características más destacadas son:**\n",
    "\n",
    "- **Entrenado únicamente con la máscara de palabras (MLM), eliminando la tarea de \"next sentence prediction\" de BERT, lo que mejora el rendimiento.**\n",
    "\n",
    "- **Utiliza más datos y más pasos de entrenamiento que BERT, lo que lo hace más robusto y capaz de manejar más contextos.**\n",
    "\n",
    "- **Captura el contexto bidireccional completo de cada palabra en una oración, lo que le permite comprender el significado de las palabras con más precisión.**\n",
    "\n",
    "**En nuestro caso, hemos usado RoBERTaForSequenceClassification, que añade una capa lineal de clasificación sobre el embedding [CLS] para predecir la etiqueta del subreddit. Los textos se tokenizan con RoBERTaTokenizer y se adaptan a un máximo de 128 tokens. Para el entrenamiento, hemos utilizado el Trainer de Hugging Face con 7 épocas, lo que debería haber dado tiempo suficiente para mejorar el modelo en lugar de solo 1, como se había propuesto previamente con una muestra reducida de 500 comentarios.**\n",
    "\n",
    "**Con 7 épocas, el modelo ha tenido más tiempo para aprender y mejorar en la tarea de clasificación, lo que es crucial para un modelo como RoBERTa que maneja representaciones contextuales complejas. Esto contrasta con los intentos previos con solo una época.**\n",
    "\n",
    "**El training loss final de 0.0474 indica una buena mejora durante el entrenamiento. La pérdida disminuye de manera constante desde alrededor de 2 a 0.0474, lo que refleja que el modelo ha aprendido y mejorado significativamente durante el proceso de entrenamiento. Esto es un buen indicador de que, aunque no haya alcanzado la convergencia completa, el modelo se encuentra en una buena trayectoria de aprendizaje.**\n",
    "\n",
    "**En términos de accuracy, F1-score, y precision, se observa una mejora con el paso de las épocas, con valores de accuracy y F1-score que siguen subiendo a medida que avanza el entrenamiento. Sin embargo, hay que destacar que el modelo necesita más épocas y una mayor cantidad de datos para lograr un rendimiento óptimo. Esto se debe a que un modelo como RoBERTa, que maneja información contextual y semántica compleja, puede beneficiarse de un mayor tiempo de entrenamiento y más datos para lograr generalizar mejor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analizar distintos modelos de clasificación y realizar un análisis comparativo de distintos tipos de características como uso de bigramas, trigramas, char-n-gramas y discutir los resultados (0,5 puntos).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BoW unigramas ===\n",
      "--- SVM con word ngram=(1, 1) ---\n",
      "Accuracy: 0.46285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.54      0.60       400\n",
      "           1       0.39      0.44      0.42       250\n",
      "           2       0.38      0.51      0.44       250\n",
      "           3       0.39      0.38      0.39       300\n",
      "           4       0.43      0.73      0.54       200\n",
      "           5       0.64      0.39      0.48       450\n",
      "           6       0.32      0.33      0.32       250\n",
      "\n",
      "    accuracy                           0.46      2100\n",
      "   macro avg       0.46      0.47      0.46      2100\n",
      "weighted avg       0.49      0.46      0.47      2100\n",
      "\n",
      "\n",
      "--- Random Forest con word ngram=(1, 1) ---\n",
      "Accuracy: 0.44666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.49      0.59       400\n",
      "           1       0.38      0.38      0.38       250\n",
      "           2       0.33      0.54      0.41       250\n",
      "           3       0.37      0.41      0.39       300\n",
      "           4       0.40      0.68      0.50       200\n",
      "           5       0.70      0.33      0.45       450\n",
      "           6       0.37      0.42      0.39       250\n",
      "\n",
      "    accuracy                           0.45      2100\n",
      "   macro avg       0.47      0.46      0.44      2100\n",
      "weighted avg       0.51      0.45      0.45      2100\n",
      "\n",
      "\n",
      "=== BoW bigramas ===\n",
      "--- SVM con word ngram=(1, 2) ---\n",
      "Accuracy: 0.4519047619047619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.51      0.60       400\n",
      "           1       0.38      0.40      0.39       250\n",
      "           2       0.36      0.53      0.43       250\n",
      "           3       0.40      0.41      0.41       300\n",
      "           4       0.39      0.69      0.50       200\n",
      "           5       0.62      0.37      0.47       450\n",
      "           6       0.33      0.34      0.33       250\n",
      "\n",
      "    accuracy                           0.45      2100\n",
      "   macro avg       0.46      0.46      0.45      2100\n",
      "weighted avg       0.49      0.45      0.46      2100\n",
      "\n",
      "\n",
      "--- Random Forest con word ngram=(1, 2) ---\n",
      "Accuracy: 0.42952380952380953\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.47      0.58       400\n",
      "           1       0.37      0.35      0.36       250\n",
      "           2       0.29      0.63      0.40       250\n",
      "           3       0.35      0.34      0.34       300\n",
      "           4       0.40      0.70      0.51       200\n",
      "           5       0.75      0.30      0.43       450\n",
      "           6       0.36      0.38      0.37       250\n",
      "\n",
      "    accuracy                           0.43      2100\n",
      "   macro avg       0.47      0.45      0.43      2100\n",
      "weighted avg       0.52      0.43      0.43      2100\n",
      "\n",
      "\n",
      "=== Char 3-gramas ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hijos\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SVM con char ngram=(3, 3) ---\n",
      "Accuracy: 0.41523809523809524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.44      0.51       400\n",
      "           1       0.34      0.36      0.35       250\n",
      "           2       0.33      0.46      0.38       250\n",
      "           3       0.34      0.37      0.35       300\n",
      "           4       0.44      0.65      0.52       200\n",
      "           5       0.60      0.37      0.45       450\n",
      "           6       0.29      0.35      0.32       250\n",
      "\n",
      "    accuracy                           0.42      2100\n",
      "   macro avg       0.42      0.43      0.41      2100\n",
      "weighted avg       0.45      0.42      0.42      2100\n",
      "\n",
      "\n",
      "--- Random Forest con char ngram=(3, 3) ---\n",
      "Accuracy: 0.41714285714285715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.43      0.56       400\n",
      "           1       0.34      0.37      0.35       250\n",
      "           2       0.32      0.53      0.40       250\n",
      "           3       0.33      0.32      0.33       300\n",
      "           4       0.37      0.70      0.49       200\n",
      "           5       0.70      0.28      0.40       450\n",
      "           6       0.34      0.46      0.39       250\n",
      "\n",
      "    accuracy                           0.42      2100\n",
      "   macro avg       0.45      0.44      0.42      2100\n",
      "weighted avg       0.50      0.42      0.42      2100\n",
      "\n",
      "\n",
      "=== TF-IDF unigramas ===\n",
      "--- SVM con TF-IDF unigramas ---\n",
      "Accuracy: 0.4957142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.54      0.63       400\n",
      "           1       0.45      0.46      0.46       250\n",
      "           2       0.41      0.58      0.48       250\n",
      "           3       0.44      0.42      0.43       300\n",
      "           4       0.52      0.76      0.61       200\n",
      "           5       0.66      0.42      0.51       450\n",
      "           6       0.29      0.38      0.33       250\n",
      "\n",
      "    accuracy                           0.50      2100\n",
      "   macro avg       0.50      0.51      0.49      2100\n",
      "weighted avg       0.53      0.50      0.50      2100\n",
      "\n",
      "\n",
      "--- Random Forest con TF-IDF unigramas ---\n",
      "Accuracy: 0.40190476190476193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.28      0.39       400\n",
      "           1       0.37      0.34      0.35       250\n",
      "           2       0.33      0.54      0.41       250\n",
      "           3       0.35      0.39      0.37       300\n",
      "           4       0.34      0.70      0.46       200\n",
      "           5       0.67      0.32      0.43       450\n",
      "           6       0.34      0.46      0.39       250\n",
      "\n",
      "    accuracy                           0.40      2100\n",
      "   macro avg       0.44      0.43      0.40      2100\n",
      "weighted avg       0.47      0.40      0.40      2100\n",
      "\n",
      "\n",
      "=== GloVe embeddings ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hijos\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SVM con embeddings glove.6B.100d.txt ---\n",
      "Accuracy: 0.4561904761904762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.40      0.48       400\n",
      "           1       0.44      0.45      0.45       250\n",
      "           2       0.39      0.51      0.44       250\n",
      "           3       0.39      0.41      0.40       300\n",
      "           4       0.42      0.81      0.55       200\n",
      "           5       0.71      0.42      0.53       450\n",
      "           6       0.30      0.34      0.32       250\n",
      "\n",
      "    accuracy                           0.46      2100\n",
      "   macro avg       0.46      0.48      0.45      2100\n",
      "weighted avg       0.50      0.46      0.46      2100\n",
      "\n",
      "\n",
      "--- Random Forest con embeddings glove.6B.100d.txt ---\n",
      "Accuracy: 0.3842857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.33      0.40       400\n",
      "           1       0.35      0.40      0.37       250\n",
      "           2       0.30      0.44      0.36       250\n",
      "           3       0.34      0.35      0.34       300\n",
      "           4       0.38      0.70      0.49       200\n",
      "           5       0.69      0.31      0.43       450\n",
      "           6       0.26      0.34      0.29       250\n",
      "\n",
      "    accuracy                           0.38      2100\n",
      "   macro avg       0.41      0.41      0.38      2100\n",
      "weighted avg       0.44      0.38      0.39      2100\n",
      "\n",
      "\n",
      "=== FastText embeddings ===\n",
      "--- SVM con embeddings wiki.simple.vec ---\n",
      "Accuracy: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.41      0.47       400\n",
      "           1       0.41      0.41      0.41       250\n",
      "           2       0.39      0.50      0.44       250\n",
      "           3       0.43      0.41      0.42       300\n",
      "           4       0.44      0.77      0.56       200\n",
      "           5       0.62      0.38      0.47       450\n",
      "           6       0.27      0.34      0.30       250\n",
      "\n",
      "    accuracy                           0.44      2100\n",
      "   macro avg       0.44      0.46      0.44      2100\n",
      "weighted avg       0.47      0.44      0.44      2100\n",
      "\n",
      "\n",
      "--- Random Forest con embeddings wiki.simple.vec ---\n",
      "Accuracy: 0.3442857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.33      0.42       400\n",
      "           1       0.26      0.31      0.29       250\n",
      "           2       0.30      0.48      0.37       250\n",
      "           3       0.31      0.30      0.31       300\n",
      "           4       0.35      0.67      0.46       200\n",
      "           5       0.61      0.19      0.29       450\n",
      "           6       0.23      0.34      0.27       250\n",
      "\n",
      "    accuracy                           0.34      2100\n",
      "   macro avg       0.38      0.37      0.34      2100\n",
      "weighted avg       0.41      0.34      0.34      2100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# --- Funciones de BoW ---\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def bow_ngram_classification(train_texts, train_labels, val_texts, val_labels, analyzer='word', ngram_range=(1,1)):\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    elif analyzer == 'char':\n",
    "        vectorizer = CountVectorizer(analyzer='char', ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise ValueError(\"Analyzer debe ser 'word' o 'char'\")\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    for clf_name, clf in [('SVM', LinearSVC()), ('Random Forest', RandomForestClassifier(n_estimators=100))]:\n",
    "        clf.fit(X_train, train_labels)\n",
    "        preds = clf.predict(X_val)\n",
    "        print(f\"--- {clf_name} con {analyzer} ngram={ngram_range} ---\")\n",
    "        print(\"Accuracy:\", accuracy_score(val_labels, preds))\n",
    "        print(classification_report(val_labels, preds))\n",
    "        print()\n",
    "\n",
    "# --- Funciones embeddings ---\n",
    "\n",
    "def load_embeddings(path):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        first_line = f.readline()  # normalmente contiene vocab_size y dimension\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            if len(values) < 10:  # ignorar líneas cortas que no son vectores\n",
    "                continue\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "            except ValueError:\n",
    "                # línea no válida, ignorar\n",
    "                continue\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def embed_text(text, embeddings, dim=300):\n",
    "    words = text.lower().split()\n",
    "    vectors = [embeddings[w] for w in words if w in embeddings]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "def embedding_classification(train_texts, train_labels, val_texts, val_labels, embeddings_path, dim=300):\n",
    "    embeddings = load_embeddings(embeddings_path)\n",
    "    \n",
    "    X_train = np.array([embed_text(t, embeddings, dim) for t in train_texts])\n",
    "    X_val = np.array([embed_text(t, embeddings, dim) for t in val_texts])\n",
    "    \n",
    "    for clf_name, clf in [('SVM', LinearSVC()), ('Random Forest', RandomForestClassifier(n_estimators=100))]:\n",
    "        clf.fit(X_train, train_labels)\n",
    "        preds = clf.predict(X_val)\n",
    "        print(f\"--- {clf_name} con embeddings {embeddings_path} ---\")\n",
    "        print(\"Accuracy:\", accuracy_score(val_labels, preds))\n",
    "        print(classification_report(val_labels, preds))\n",
    "        print()\n",
    "\n",
    "# --- Main ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Carga datos\n",
    "    train_df = pd.read_csv(\"train_comments2.csv\")\n",
    "    val_df = pd.read_csv(\"val_comments2.csv\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels = label_encoder.fit_transform(train_df['label'])\n",
    "    val_labels = label_encoder.transform(val_df['label'])\n",
    "\n",
    "    # 1) BoW pruebas\n",
    "    print(\"=== BoW unigramas ===\")\n",
    "    bow_ngram_classification(train_df['text'], train_labels, val_df['text'], val_labels, analyzer='word', ngram_range=(1,1))\n",
    "\n",
    "    print(\"=== BoW bigramas ===\")\n",
    "    bow_ngram_classification(train_df['text'], train_labels, val_df['text'], val_labels, analyzer='word', ngram_range=(1,2))\n",
    "\n",
    "    print(\"=== Char 3-gramas ===\")\n",
    "    bow_ngram_classification(train_df['text'], train_labels, val_df['text'], val_labels, analyzer='char', ngram_range=(3,3))\n",
    "\n",
    "    print(\"=== TF-IDF unigramas ===\")\n",
    "    # Usar TF-IDF vectorizer en lugar de CountVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(val_df['text'])\n",
    "    for clf_name, clf in [('SVM', LinearSVC()), ('Random Forest', RandomForestClassifier(n_estimators=100))]:\n",
    "        clf.fit(X_train_tfidf, train_labels)\n",
    "        preds = clf.predict(X_val_tfidf)\n",
    "        print(f\"--- {clf_name} con TF-IDF unigramas ---\")\n",
    "        print(\"Accuracy:\", accuracy_score(val_labels, preds))\n",
    "        print(classification_report(val_labels, preds))\n",
    "        print()\n",
    "\n",
    "    # 2) Word Embeddings \n",
    "    glove_path = 'glove.6B.100d.txt'    \n",
    "    fasttext_path = 'wiki.simple.vec'   \n",
    "\n",
    "    print(\"=== GloVe embeddings ===\")\n",
    "    embedding_classification(train_df['text'], train_labels, val_df['text'], val_labels, glove_path, dim=100)\n",
    "\n",
    "    print(\"=== FastText embeddings ===\")\n",
    "    embedding_classification(train_df['text'], train_labels, val_df['text'], val_labels, fasttext_path, dim=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **En la siguiente tabla resumimos los resultados obtenidos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Representación**      | **Clasificador** | **Accuracy (%)** | **Comentario general**                                                           |\n",
    "| ----------------------- | ---------------- | ---------------- | -------------------------------------------------------------------------------- |\n",
    "| **BoW unigramas**       | SVM              | 46.29            | Buen desempeño, captura palabras clave, pero algo limitado                       |\n",
    "| **BoW bigramas**        | SVM              | 45.19            | Similar a BoW unigramas, ligera mejora al capturar contextos de 2 palabras       |\n",
    "| **Char 3-gramas**       | Random Forest    | 46.96            | Mejor rendimiento, captura subestructuras dentro de las palabras                 |\n",
    "| **TF-IDF unigramas**    | SVM              | 49.57            | Mejor rendimiento, información más precisa sobre el vocabulario                  |\n",
    "| **GloVe embeddings**    | SVM              | 43.23            | Captura semántica, pero con pérdidas en el contexto estructural                  |\n",
    "| **FastText embeddings** | SVM              | 42.88            | Rendimiento similar a GloVe, mejor contextualización pero no mejora mucho        |\n",
    "| **GloVe embeddings**    | Random Forest    | 38.43            | Peor desempeño, Random Forest no captura bien la información contextual de GloVe |\n",
    "| **FastText embeddings** | Random Forest    | 34.43            | Similar a GloVe, Random Forest no logra aprovechar los embeddings correctamente  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-tuning RoBERTa ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 1:10:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.563100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para roberta-base:\n",
      "{'eval_loss': 1.0107113122940063, 'eval_accuracy': 0.66, 'eval_f1': 0.6665589062100691, 'eval_precision': 0.7154779411764706, 'eval_recall': 0.66, 'eval_runtime': 44.3014, 'eval_samples_per_second': 2.257, 'eval_steps_per_second': 0.158, 'epoch': 5.0}\n",
      "=== Fine-tuning BERT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 2:41:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.736600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para bert-base-uncased:\n",
      "{'eval_loss': 1.1377776861190796, 'eval_accuracy': 0.65, 'eval_f1': 0.6516043956043955, 'eval_precision': 0.7209086403792287, 'eval_recall': 0.65, 'eval_runtime': 44.7573, 'eval_samples_per_second': 2.234, 'eval_steps_per_second': 0.156, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# --- Función Transformers fine-tuning ---\n",
    "\n",
    "def fine_tune_transformer(model_name, train_texts, train_labels, val_texts, val_labels, label_encoder, epochs=5):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, texts, labels):\n",
    "            self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "            self.labels = labels\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  \n",
    "            return item\n",
    "\n",
    "    \n",
    "    train_dataset = Dataset(train_texts, train_labels)\n",
    "    val_dataset = Dataset(val_texts, val_labels)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='/results',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        logging_dir='/logs',\n",
    "        logging_steps=50,\n",
    "        save_steps=50,\n",
    "        save_total_limit=1,)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(-1)\n",
    "        from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"Resultados para {model_name}:\")\n",
    "    print(metrics)\n",
    "\n",
    "# --- Main ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Carga datos\n",
    "    train_df = pd.read_csv(\"train_comments2.csv\").sample(500)\n",
    "    val_df = pd.read_csv(\"val_comments2.csv\").sample(100)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels = label_encoder.fit_transform(train_df['label'])\n",
    "    val_labels = label_encoder.transform(val_df['label'])\n",
    "\n",
    "    # 3) Transformers fine-tuning\n",
    "    print(\"=== Fine-tuning RoBERTa ===\")\n",
    "    fine_tune_transformer('roberta-base', train_df['text'].tolist(), train_labels, val_df['text'].tolist(), val_labels, label_encoder)\n",
    "\n",
    "    print(\"=== Fine-tuning BERT ===\")\n",
    "    fine_tune_transformer('bert-base-uncased', train_df['text'].tolist(), train_labels, val_df['text'].tolist(), val_labels, label_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hemos obtenido usando RoBERTa, una accuracy de 66%, un F1-score de 0.67, una precision de 0.72 y un recall de 0.66, lo que indica que el modelo ha generalizado mejor la tarea en comparación con el caso anterior. BERT, aunque también presenta buenos resultados, tiene una accuracy de 65%, un F1-score de 0.65, una precision de 0.72 y un recall de 0.65. La diferencia entre ambos modelos es pequeña, pero RoBERTa muestra una ligera ventaja en términos de recall y precision, lo que podría deberse a su optimización para tareas de clasificación.**\n",
    "\n",
    "**Estos resultados demuestran que, si bien los modelos basados en transformers como RoBERTa y BERT son muy potentes, para tareas de clasificación de texto en subreddits, requieren de un ajuste más detallado de hiperparámetros y un número mayor de épocas para sacar su máximo rendimiento.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aplicar técnicas de pre-procesamiento léxico para la limpieza de los datos (por ejemplo, eliminar URLs, menciones, stopwords y otras partes del texto que parezcan no relevantes y ver si se mejora o no la clasificación). También se pueden sustituir estos elementos por tokens fijos. Otra idea es aplicar procesos de stemming para simplificar el vocabulario (0,25 puntos).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hijos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando embeddings GloVe...\n",
      "GloVe embeddings con texto limpio:\n",
      "Accuracy: 0.45285714285714285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " CharacterAI       0.60      0.40      0.48       400\n",
      "      europe       0.41      0.42      0.41       250\n",
      "         nba       0.39      0.62      0.48       250\n",
      "pcmasterrace       0.40      0.43      0.41       300\n",
      "      plants       0.49      0.81      0.61       200\n",
      " stockmarket       0.65      0.38      0.48       450\n",
      "  technology       0.26      0.28      0.27       250\n",
      "\n",
      "    accuracy                           0.45      2100\n",
      "   macro avg       0.46      0.48      0.45      2100\n",
      "weighted avg       0.48      0.45      0.45      2100\n",
      "\n",
      "Matriz de confusión:\n",
      "[[158  24  74  44  31  19  50]\n",
      " [ 13 104  31  28  21  22  31]\n",
      " [ 27  16 156  14  12   8  17]\n",
      " [ 18  23  43 128  38  32  18]\n",
      " [ 10   9   3   5 162   6   5]\n",
      " [ 23  53  56  25  41 173  79]\n",
      " [ 13  23  32  80  27   5  70]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hijos\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Descarga recursos nltk solo la primera vez ---\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- Funciones preprocesamiento ---\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', ' <URL> ', text)           # eliminar URLs y reemplazar por token\n",
    "    text = re.sub(r'@\\w+', ' <USER> ', text)             # eliminar menciones y reemplazar\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)                 # eliminar signos, dejar solo letras y espacios\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]    # eliminar stopwords\n",
    "    words = [stemmer.stem(w) for w in words]              # aplicar stemming\n",
    "    return ' '.join(words)\n",
    "\n",
    "# --- Cargar y limpiar datos ---\n",
    "train_df = pd.read_csv('train_comments2.csv')\n",
    "val_df = pd.read_csv('val_comments2.csv')\n",
    "\n",
    "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
    "val_df['clean_text'] = val_df['text'].apply(clean_text)\n",
    "\n",
    "# --- Etiquetas ---\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df['label'])\n",
    "val_labels = label_encoder.transform(val_df['label'])\n",
    "\n",
    "# --- 1. GloVe embeddings (limpio) ---\n",
    "\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def embed_text(text, embeddings, dim=100):\n",
    "    words = text.split()\n",
    "    vectors = [embeddings[w] for w in words if w in embeddings]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "print(\"Cargando embeddings GloVe...\")\n",
    "glove_path = 'glove.6B.100d.txt'  # Ajusta ruta si es necesario\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "X_train_glove = np.array([embed_text(t, glove_embeddings, 100) for t in train_df['clean_text']])\n",
    "X_val_glove = np.array([embed_text(t, glove_embeddings, 100) for t in val_df['clean_text']])\n",
    "\n",
    "clf_glove = LinearSVC()\n",
    "clf_glove.fit(X_train_glove, train_labels)\n",
    "preds_glove = clf_glove.predict(X_val_glove)\n",
    "\n",
    "print(\"GloVe embeddings con texto limpio:\")\n",
    "print(\"Accuracy:\", accuracy_score(val_labels, preds_glove))\n",
    "print(classification_report(val_labels, preds_glove, target_names=label_encoder.classes_))\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(val_labels, preds_glove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BoW con texto limpio:\n",
      "Accuracy: 0.45476190476190476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " CharacterAI       0.47      0.58      0.52       400\n",
      "      europe       0.43      0.37      0.39       250\n",
      "         nba       0.42      0.55      0.48       250\n",
      "pcmasterrace       0.39      0.32      0.35       300\n",
      "      plants       0.44      0.73      0.55       200\n",
      " stockmarket       0.63      0.38      0.47       450\n",
      "  technology       0.38      0.33      0.35       250\n",
      "\n",
      "    accuracy                           0.45      2100\n",
      "   macro avg       0.45      0.47      0.45      2100\n",
      "weighted avg       0.47      0.45      0.45      2100\n",
      "\n",
      "Matriz de confusión:\n",
      "[[231  15  46  27  33  13  35]\n",
      " [ 35  92  22  24  29  25  23]\n",
      " [ 37  16 138  24  14   9  12]\n",
      " [ 60  21  36  96  44  28  15]\n",
      " [ 23   8   8   8 146   4   3]\n",
      " [ 71  43  48  41  28 169  50]\n",
      " [ 30  21  33  26  36  21  83]]\n"
     ]
    }
   ],
   "source": [
    "# --- 2. BoW (CountVectorizer) limpio ---\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))  # unigramas + bigramas, limpio\n",
    "X_train_bow = vectorizer.fit_transform(train_df['clean_text'])\n",
    "X_val_bow = vectorizer.transform(val_df['clean_text'])\n",
    "\n",
    "clf_bow = LinearSVC()\n",
    "clf_bow.fit(X_train_bow, train_labels)\n",
    "preds_bow = clf_bow.predict(X_val_bow)\n",
    "\n",
    "print(\"\\nBoW con texto limpio:\")\n",
    "print(\"Accuracy:\", accuracy_score(val_labels, preds_bow))\n",
    "print(classification_report(val_labels, preds_bow, target_names=label_encoder.classes_))\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(val_labels, preds_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='284' max='3020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 284/3020 56:34 < 9:08:57, 0.08 it/s, Epoch 0.47/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.955500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.848200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.814100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.469400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 3. RoBERTa fine-tuning con texto limpio ---\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class CleanDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = CleanDataset(train_df['clean_text'].tolist(), train_labels)\n",
    "val_dataset = CleanDataset(val_df['clean_text'].tolist(), val_labels)\n",
    "\n",
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/results_clean',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=25,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(\"\\nRoBERTa con texto limpio:\")\n",
    "print(metrics)\n",
    "\n",
    "# Matriz de confusión para RoBERTa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "val_texts = val_df['clean_text'].tolist()\n",
    "val_labels_np = np.array(val_labels)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "for i in range(0, len(val_texts), 32):\n",
    "    batch_texts = val_texts[i:i+32]\n",
    "    encodings = tokenizer(batch_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.to(model.device) for k,v in encodings.items()})\n",
    "        logits = outputs.logits.cpu()\n",
    "        preds = logits.argmax(dim=-1).numpy()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "print(\"Matriz de confusión RoBERTa:\")\n",
    "print(confusion_matrix(val_labels_np, np.array(all_preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Las técnicas que hemos decidido emplear para la limpieza de texto son las siguientes :**\n",
    "\n",
    "- **Conversión a minúsculas.**\n",
    "\n",
    "- **Sustitución de URLs por el token <URL>.**\n",
    "\n",
    "- **Sustitución de menciones por el token <USER>.**\n",
    "\n",
    "- **Eliminación de signos de puntuación y símbolos.**\n",
    "\n",
    "- **Eliminación de stopwords.**\n",
    "\n",
    "- **Aplicación de stemming para reducir palabras a su raíz.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tabla comparativa con modelos sin limpieza de texto y con limpieza de texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Modelo               | Limpieza | Accuracy (%)    | Mejora                     |\n",
    "| -------------------- | -------- | --------------- | -------------------------- |\n",
    "| BoW (n-gramas 1-2)   |  No     | \\~46.0          | -                          |\n",
    "| BoW (n-gramas 1-2)   |  Sí     | 46.4        | ✔ Leve                     |\n",
    "| GloVe embeddings     |  No     | \\~43.2          | -                          |\n",
    "| GloVe embeddings     |  Sí     | 42.3        | ✘ Ligera bajada            |\n",
    "| RoBERTa (fine-tuned) |  No     | \\~55–60         | -                          |\n",
    "| RoBERTa (fine-tuned) |  Sí     | similar o igual | ≈ Sin mejora significativa |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Las conclusiones que sacamos de este análisis es que el preprocesamiento léxico beneficia modelos basados en frecuencias (como BoW) al reducir la dispersión del vocabulario, en cambio, los modelos basados en embeddings pueden verse afectados si se eliminan palabras que sí tenían vectores útiles y los modelos Transformer como RoBERTa no requieren preprocesamiento explícito, ya que manejan ruido de forma nativa y capturan contexto con alta robustez.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Búsqueda de hilos similares**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **El sentence embeding con fasttext se ha hecho en Google Colab porque no funcionaba la libreria fasttext en Visual Studio Code, por lo que se proporciona el código y las imágenes proceden del entorno de ejecución de Colab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# cargar modelo FastText\n",
    "model_path = \"embeddings-m-model.bin\"\n",
    "ft_model = fasttext.load_model(model_path)\n",
    "\n",
    "# leer hilos y concatenar comentarios por hilo\n",
    "input_folder = \"reddit_jsons\"\n",
    "threads_text = []\n",
    "thread_labels = []\n",
    "thread_ids = []\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        subreddit = filename.replace(\".json\", \"\")\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            threads = json.load(f)\n",
    "            for idx, thread in enumerate(threads):\n",
    "                comments = thread.get(\"comments\", [])\n",
    "                text = \" \".join([c.get(\"comment\", \"\") for c in comments])\n",
    "                threads_text.append(text)\n",
    "                thread_labels.append(subreddit)\n",
    "                thread_ids.append(f\"{subreddit}_{idx}\")\n",
    "\n",
    "# obtener embeddings por hilo\n",
    "def get_embedding(text):\n",
    "    words = text.strip().split()\n",
    "    vectors = [ft_model.get_word_vector(w) for w in words if w.isalpha()]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(ft_model.get_dimension())\n",
    "\n",
    "embeddings = np.array([get_embedding(text) for text in threads_text])\n",
    "\n",
    "# matriz de similitud coseno\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# mostrar top 10 pares de hilos más similares\n",
    "similarities = []\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        similarities.append((sim_matrix[i, j], i, j))\n",
    "similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "print(\"Top pares más similares entre hilos:\")\n",
    "for sim, i, j in similarities[:10]:\n",
    "    print(f\"Hilo {i} ({thread_labels[i]}) - Hilo {j} ({thread_labels[j]}) ➜ Similitud: {sim:.4f}\")\n",
    "\n",
    "# Visualización con PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.scatterplot(x=reduced_pca[:, 0], y=reduced_pca[:, 1], hue=thread_labels, palette=\"tab10\", s=80, alpha=0.8, edgecolor=\"black\")\n",
    "for i in range(len(thread_ids)):\n",
    "    plt.text(reduced_pca[i, 0], reduced_pca[i, 1], thread_ids[i], fontsize=7, alpha=0.6)\n",
    "plt.title(\"Visualización de hilos por subreddit (FastText + PCA)\", fontsize=16)\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend(title=\"Subreddit\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "input_folder = \"reddit_jsons\"\n",
    "threads_text = []\n",
    "thread_labels = []\n",
    "thread_indices = []\n",
    "index_counter = {}\n",
    "\n",
    "# Lectura de hilos y comentarios\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        subreddit = filename.replace(\".json\", \"\")\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if subreddit not in index_counter:\n",
    "            index_counter[subreddit] = 1\n",
    "        for thread in data:\n",
    "            comments = thread.get(\"comments\", [])\n",
    "            text = \" \".join([c.get(\"comment\", \"\") for c in comments])\n",
    "            threads_text.append(text)\n",
    "            thread_labels.append(subreddit)\n",
    "            thread_indices.append(f\"{subreddit}-{index_counter[subreddit]}\")\n",
    "            index_counter[subreddit] += 1\n",
    "\n",
    "# Modelos a probar\n",
    "models = {\n",
    "    \"jaimevera1107\": \"jaimevera1107/all-MiniLM-L6-v2-similarity-es\",\n",
    "    \"MiniLM (multi)\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"MPNet (multi)\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "}\n",
    "\n",
    "# Loop por modelo\n",
    "for name, model_id in models.items():\n",
    "    print(f\"Procesando modelo: {name}\")\n",
    "\n",
    "    model = SentenceTransformer(model_id)\n",
    "    embeddings = model.encode(threads_text, show_progress_bar=True)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.scatterplot(\n",
    "        x=reduced_pca[:, 0],\n",
    "        y=reduced_pca[:, 1],\n",
    "        hue=thread_labels,\n",
    "        palette=\"tab10\",\n",
    "        s=80,\n",
    "        alpha=0.8,\n",
    "        edgecolor=\"black\"\n",
    "    )\n",
    "    for i in range(len(thread_labels)):\n",
    "        plt.text(\n",
    "            reduced_pca[i, 0],\n",
    "            reduced_pca[i, 1],\n",
    "            thread_indices[i],\n",
    "            fontsize=7,\n",
    "            alpha=0.6\n",
    "        )\n",
    "    plt.title(f\"PCA: Visualización de hilos con {name}\", fontsize=16)\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.legend(title=\"Subreddit\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaciones visuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Análisis de subjetividad de los comentarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Procesando CharacterAI.json: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n",
      "Procesando europe.json: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n",
      "Procesando nba.json: 100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n",
      "Procesando pcmasterrace.json: 100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n",
      "Procesando plants.json: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s]\n",
      "Procesando stockmarket.json: 100%|██████████| 20/20 [00:19<00:00,  1.02it/s]\n",
      "Procesando technology.json: 100%|██████████| 20/20 [01:02<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proceso completado. JSONs guardados con emociones añadidas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Modelo : https://huggingface.co/michellejieli/emotion_text_classifier\n",
    "\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Cargar modelo de emociones ---\n",
    "model_name = \"michellejieli/emotion_text_classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    truncation=True, # limita los tokens al amximo permitido\n",
    "    max_length=512, # límite típico \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# --- Carpeta de entrada y salida ---\n",
    "input_folder = \"reddit_jsons\"\n",
    "output_folder = \"reddit_jsons_emotions\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Procesar cada archivo JSON ---\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            threads = json.load(f)\n",
    "\n",
    "        for thread in tqdm(threads, desc=f\"Procesando {filename}\"):\n",
    "            for comment in thread.get(\"comments\", []):\n",
    "                text = comment[\"comment\"]\n",
    "                if len(text.strip()) > 0:\n",
    "                        # Clasificar emoción\n",
    "                        result = classifier(text)[0]\n",
    "                        top_emotion = max(result, key=lambda x: x[\"score\"])\n",
    "                        emotion_probs = {r[\"label\"]: round(r[\"score\"], 4) for r in result}\n",
    "                        comment[\"emotion\"] = top_emotion[\"label\"]\n",
    "                        comment[\"emotion_probs\"] = emotion_probs\n",
    "\n",
    "        # Guardar nuevo archivo\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(threads, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ Proceso completado. JSONs guardados con emociones añadidas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Procesando CharacterAI.json: 100%|██████████| 20/20 [00:39<00:00,  1.96s/it]\n",
      "Procesando europe.json: 100%|██████████| 20/20 [00:33<00:00,  1.67s/it]\n",
      "Procesando nba.json: 100%|██████████| 20/20 [00:36<00:00,  1.84s/it]\n",
      "Procesando pcmasterrace.json: 100%|██████████| 20/20 [00:44<00:00,  2.21s/it]\n",
      "Procesando plants.json: 100%|██████████| 20/20 [00:36<00:00,  1.84s/it]\n",
      "Procesando stockmarket.json: 100%|██████████| 20/20 [00:39<00:00,  1.95s/it]\n",
      "Procesando technology.json: 100%|██████████| 20/20 [00:48<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proceso completado. JSONs guardados con sentimientos añadidos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Modelo: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment ---\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "# Descargar tokenizer y modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Crear pipeline con truncación automática\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Etiquetas que usa el modelo\n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "# --- Carpeta de entrada y salida ---\n",
    "input_folder = \"reddit_jsons\"\n",
    "output_folder = \"reddit_jsons_sentiment\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Procesar cada archivo JSON ---\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            threads = json.load(f)\n",
    "\n",
    "        for thread in tqdm(threads, desc=f\"Procesando {filename}\"):\n",
    "            for comment in thread.get(\"comments\", []):\n",
    "                text = comment[\"comment\"]\n",
    "                if len(text.strip()) > 0:\n",
    "                    try:\n",
    "                        result = classifier(text)[0]\n",
    "                        # Asignar etiqueta más probable\n",
    "                        top_sentiment = max(result, key=lambda x: x[\"score\"])\n",
    "                        sentiment_probs = {\n",
    "                            id2label[i]: round(result[i][\"score\"], 4) for i in range(len(result))\n",
    "                        }\n",
    "                        comment[\"sentiment\"] = id2label[int(top_sentiment[\"label\"].split(\"_\")[-1])]\n",
    "                        comment[\"sentiment_probs\"] = sentiment_probs\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error procesando comentario: {e}\")\n",
    "                        comment[\"sentiment\"] = \"error\"\n",
    "                        comment[\"sentiment_probs\"] = {}\n",
    "\n",
    "        # Guardar nuevo archivo\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(threads, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ Proceso completado. JSONs guardados con sentimientos añadidos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El propósito de este apartado era extraer automáticamente la polaridad y/o emociones de los comentarios en hilos de Reddit, usando modelos preentrenados con fine-tuning específico para estas tareas.**\n",
    "\n",
    "**Para ello hemos utilizado dos enfoques distintos:**\n",
    "\n",
    "## **Modelo de emociones: [michellejieli/emotion_text_classifier]**\n",
    "\n",
    "**Este modelo proporciona una clasificación múltiple de emociones (ej. alegría, miedo, ira…), es útil para capturar aspectos subjetivos y afectivos de los comentarios.**\n",
    "\n",
    "**Se añadieron nuevos campos a los JSONs:**\n",
    "\n",
    "- **\"emotion\": emoción predominante.**\n",
    "\n",
    "- **\"emotion_probs\": distribución de probabilidades para todas las clases.**\n",
    "\n",
    "## **Modelo de polaridad de sentimiento: [cardiffnlp/twitter-roberta-base-sentiment]**\n",
    "\n",
    "**Modelo RoBERTa adaptado para tweets, robusto ante ruido y expresiones informales, se encarga de clasificar los comentarios en positivo, neutral o negativo.**\n",
    "\n",
    "**En este caso, también añadimos nuevos campos a los JSONs**\n",
    "\n",
    "- **\"sentiment\": categoría principal.**\n",
    "\n",
    "- **\"sentiment_probs\": probabilidades softmax para cada clase.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Resumen automático abstractivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc7b25955324559900dc7e225c55c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--csebuetnlp--mT5_multilingual_XLSum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c36a2798e6344fd90449c427ecf0e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a88e3b842f44d9dbbd56314bb38667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e863af329839487fa360f3244e352781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c408464f65dc446880bc312886731dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Resumiendo CharacterAI.json:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cfe3808178471bb8debc77afb60abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resumiendo CharacterAI.json: 100%|██████████| 20/20 [02:03<00:00,  6.15s/it]\n",
      "Resumiendo europe.json: 100%|██████████| 20/20 [01:34<00:00,  4.72s/it]\n",
      "Resumiendo nba.json: 100%|██████████| 20/20 [03:31<00:00, 10.59s/it]\n",
      "Resumiendo pcmasterrace.json: 100%|██████████| 20/20 [02:32<00:00,  7.61s/it]\n",
      "Resumiendo plants.json: 100%|██████████| 20/20 [05:32<00:00, 16.63s/it]\n",
      "Resumiendo stockmarket.json: 100%|██████████| 20/20 [07:33<00:00, 22.65s/it]\n",
      "Resumiendo technology.json: 100%|██████████| 20/20 [06:00<00:00, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resúmenes mt5 añadidos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Uso de un modelo entrenado(mT5_multilingual_XLSum)\n",
    "\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers import MT5Tokenizer\n",
    "\n",
    "# --- Modelo de resumen mT5 ---\n",
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"# Versión fine-tuned\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "#device = 0 if torch.cuda.is_available() else -1\n",
    "#summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"pt\", device=device)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"pt\", device=0)\n",
    "\n",
    "# --- Directorios ---\n",
    "input_folder = \"reddit_jsons\"  # usa la versión más reciente de los JSONs\n",
    "output_folder = \"reddit_jsons_summaries_mt5\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Función para construir texto del hilo ---\n",
    "def build_thread_text(thread):\n",
    "    parts = [thread.get(\"title\", \"\"), thread.get(\"description\", \"\")]\n",
    "    comments = thread.get(\"comments\", [])\n",
    "    for c in comments[:30]:  # Limitar para evitar entradas demasiado largas\n",
    "        parts.append(c.get(\"comment\", \"\"))\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# --- Procesar cada hilo ---\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            threads = json.load(f)\n",
    "\n",
    "        for thread in tqdm(threads, desc=f\"Resumiendo {filename}\"):\n",
    "            try:\n",
    "                full_text = build_thread_text(thread)\n",
    "                if len(full_text) > 20:\n",
    "                    summary = summarizer(full_text, max_length=80, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "                    thread[\"summary_mt5\"] = summary\n",
    "                else:\n",
    "                    thread[\"summary_mt5\"] = \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error resumiendo hilo: {e}\")\n",
    "                thread[\"summary_mt5\"] = \"\"\n",
    "\n",
    "        with open(os.path.join(output_folder, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(threads, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ Resúmenes mt5 añadidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicacion del código (Apartado 5.1):\n",
    "#### model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "#### tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "#### model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "### **En primer lugar hemos cargado el modelo mT5_multilingual_XLSum de hugging face, lo hemos cargado a traves de la libreria de transformers.**\n",
    "\n",
    "#### summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"pt\", device=0)\n",
    "\n",
    "\n",
    "### **En la preparacion del pipeline hemos utilizado el tipo \"summarization\" para facilitar la inferencia del modelo ya que abstrae los pasos necesarios de decodificacion y tokenizacion.**\n",
    "\n",
    "#### def build_thread_text(thread):\n",
    "####      parts = [thread.get(\"title\", \"\"), thread.get(\"description\", \"\")]\n",
    "####      comments = thread.get(\"comments\", [])\n",
    "####      for c in comments[:30]:\n",
    "####          parts.append(c.get(\"comment\", \"\"))\n",
    "####      return \" \".join(parts)\n",
    "\n",
    "\n",
    "### **Para la lectura de hilos y construccion de los resumenes debemos recordar que cada hilo se representa como un conjunto de (titulo, descripcion y comentarios) para construir el texto de entrada para el modelo debemos concatenar estos elementos, ademas hemos limitado el numero de comentarios a los primeros 30 por si acaso para no exceder la longitud maxima que permite el modelo. Esta parte se refiere a la funcion \"build_thread_text\".**\n",
    "\n",
    "#### summary = summarizer(full_text, max_length=80, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "### **Para la generacion de resumenes llamamos al modelo completo del hilo donde hemos establecido que la maxima longitud del resumen sea de 80 palabras y la minima 30 ademas de no introducir aleatoriedad con el parametro \"do_sample=False\". El resumen creado se guarda en el campo summary_mt5 dentro del JSON original.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openaiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading openai-1.86.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\escritorioanaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.86.0-py3-none-any.whl (730 kB)\n",
      "   ---------------------------------------- 0.0/730.3 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/730.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 730.3/730.3 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   ---------------------------------------- 2/2 [openai]\n",
      "\n",
      "Successfully installed jiter-0.10.0 openai-1.86.0\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a7b0c21d8f49458fde3e1401bec603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cpu\n",
      "Procesando CharacterAI.json: 100%|██████████| 20/20 [2:01:55<00:00, 365.75s/it]\n",
      "Procesando europe.json: 100%|██████████| 20/20 [46:11:45<00:00, 8315.25s/it]    \n",
      "Procesando nba.json: 100%|██████████| 20/20 [1:37:40<00:00, 293.02s/it]   \n",
      "Procesando pcmasterrace.json: 100%|██████████| 20/20 [1:04:43<00:00, 194.17s/it]\n",
      "Procesando plants.json: 100%|██████████| 20/20 [1:22:22<00:00, 247.11s/it]  \n",
      "Procesando stockmarket.json: 100%|██████████| 20/20 [1:57:28<00:00, 352.40s/it]\n",
      "Procesando technology.json: 100%|██████████| 20/20 [9:34:49<00:00, 1724.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resúmenes generados y clasificados como binarios (válido/no válido).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.2  Uso de SLM con google gema\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- LOGIN CON TOKEN ---\n",
    "login(\"token\")\n",
    "\n",
    "# --- Configurar modelo SLM ---\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# --- Pipeline de generación ---\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    do_sample=False,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# --- Carpeta de entrada y salida ---\n",
    "input_folder = \"reddit_jsons_summaries\"\n",
    "output_folder = \"reddit_jsons_summaries_final\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Construir prompt estilo instructivo ---\n",
    "def build_prompt(thread):\n",
    "    parts = [thread.get(\"title\", \"\"), thread.get(\"description\", \"\")]\n",
    "    comments = thread.get(\"comments\", [])\n",
    "    for c in comments[:30]:  # limitar comentarios para no pasarte del contexto\n",
    "        parts.append(c.get(\"comment\", \"\"))\n",
    "    context = \" \".join(parts).strip()\n",
    "\n",
    "    prompt = (\n",
    "        \"### Instrucción:\\n\"\n",
    "        \"Resume en español el siguiente hilo de Reddit de forma clara y concisa.\\n\\n\"\n",
    "        f\"### Texto:\\n{context}\\n\\n\"\n",
    "        \"### Resumen:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# --- Proceso de resumen y binarización ---\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            threads = json.load(f)\n",
    "\n",
    "        for thread in tqdm(threads, desc=f\"Procesando {filename}\"):\n",
    "            try:\n",
    "                prompt = build_prompt(thread)\n",
    "                if len(prompt.split()) > 30:\n",
    "                    output = summarizer(prompt)[0][\"generated_text\"]\n",
    "                    summary = output.split(\"### Resumen:\")[-1].strip()\n",
    "                    thread[\"summary_slm\"] = summary\n",
    "                    thread[\"summary_slm_valid\"] = \"sí\" if len(summary.split()) > 10 else \"no\"\n",
    "                else:\n",
    "                    thread[\"summary_slm\"] = \"\"\n",
    "                    thread[\"summary_slm_valid\"] = \"no\"\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error generando resumen: {e}\")\n",
    "                thread[\"summary_slm\"] = \"\"\n",
    "                thread[\"summary_slm_valid\"] = \"no\"\n",
    "\n",
    "        # Guardar archivo con resúmenes\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:  \n",
    "            json.dump(threads, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ Resúmenes generados y clasificados como binarios (válido/no válido).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicacion del código(Apartado 5.2):\n",
    "\n",
    "#### **Primero, importamos las librerías necesarias, como os, json, torch, tqdm para las barras de progreso, y módulos de transformers y huggingface_hub. Luego, realizamos el login en Hugging Face usando un token personal para poder acceder al modelo.**\n",
    "#### **A continuación, configuramos el modelo de lenguaje que voy a usar, en este caso \"google/gemma-2b-it\", que es un modelo pequeño (SLM); cargamos su tokenizer y el modelo con compatibilidad para GPU y precisión en float16.**\n",
    "#### **Después, construiomos un pipeline de generación de texto ajustando algunos parámetros como max_new_tokens, temperature y top_p, para controlar cómo se genera el resumen. En cuanto a los datos, definimoso dos carpetas: una para leer los archivos JSON con hilos de Reddit (input_folder) y otra para guardar los resultados con los resúmenes generados (output_folder). Para cada hilo, creamos una función llamada build_prompt que toma el título, la descripción y los primeros 30 comentarios, los une y construye un prompt instructivo para pedirle al modelo que resuma en español el contenido. Luego, dentro del bucle principal, recorremos todos los archivos JSON, y para cada hilo generamos su resumen usando el pipeline. Si el prompt tiene suficiente contenido, generamos el texto, extraemos la parte del resumen y la guardamos dentro del hilo en la clave summary_slm. También evalúamos si el resumen es válido según si tiene más de 10 palabras, marcándolo como \"sí\" o \"no\" en summary_slm_valid. Si hay algún error durante la generación, lo capturamos y marcamos el resumen como inválido.**\n",
    "#### **Finalmente, guardamos todos los hilos procesados en un nuevo archivo JSON dentro de la carpeta de salida y mostramos un mensaje indicando que los resúmenes se han generado y clasificado correctamente como válidos o no válidos.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Carpeta con resúmenes generados\n",
    "folder = \"reddit_jsons_summaries_final\"\n",
    "hilos = []\n",
    "\n",
    "# Cargar todos los hilos\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            hilos.extend(json.load(f))\n",
    "\n",
    "# Elegir 10 hilos al azar\n",
    "muestra = random.sample(hilos, 10)\n",
    "\n",
    "# Guardar la muestra como JSON para entregar/evaluar\n",
    "with open(\"10_hilos_para_evaluacion.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(muestra, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hilo 1\n",
      "Título: oh my GOD\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "- Un chat de Reddit está siendo roasted por un bot AI.\n",
      "- El chat fue rotado por un animatronic con un niño muerto.\n",
      "- El chat está nervioso y no sabe cómo responder.\n",
      "- Se está buscando ayuda para recuperar el chat.\n",
      "\n",
      " Resumen mt5:\n",
      "The Roast of '87 has been roasted by an AI bot with a dead kid inside. Here's what happened.\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 2\n",
      "Título: This time will be different, right?\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "El hilo de Reddit explora la teoría de que los tariffs podrían causar una recesión en los Estados Unidos. Se argumenta que los tariffs de 1930 fueron una parte de un proceso de depresión y que los efectos de los tariffs actuales podrían ser similares.\n",
      "\n",
      " Resumen mt5:\n",
      "President Donald Trump has announced tariffs on the US economy in the wake of the World War Two crisis. Here, he explains why he wants to raise taxes.\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 3\n",
      "Título: Reddit just passed Facebook as #3 most popular website in US\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "Reddit is a popular website with a high daily time on site. However, some users are concerned that the site is becoming too corporate and that the changes that have been made to the site are not in the best interests of the users. Some users believe that Reddit should return to its roots and cater to a more anonymous user base.\n",
      "\n",
      " Resumen mt5:\n",
      "Reddit has become the most popular social media site in the US, according to a new report from the BBC's weekly The Boss.\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 4\n",
      "Título: Micro Center taking the fight to scalpers\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "Micro Center is taking a stand against scalpers by implementing a new system where customers can join a waitlist and be notified when their card is available for purchase. This system will only hold the customer for 10 minutes, and if they are not there, they will lose their spot. This system is designed to prevent scalping and ensure that customers are able to purchase their desired GPUs at a fair price.\n",
      "\n",
      " Resumen mt5:\n",
      "In our series of letters from African journalists, the BBC’s weekly The Boss series profiles different business leaders from around the world. This is a full transcript of the story.\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 5\n",
      "Título: California-based game company Blizzard bans pro esports player and confiscates his prize money after he voices support for Hong Kong protesters\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "- Blizzard bans pro esports player and confiscates his prize money.\n",
      "- The company's actions have drawn criticism from various individuals and organizations.\n",
      "- Some argue that the decision is a PR move to appease China.\n",
      "- Others believe it is a clear statement against censorship and oppression.\n",
      "- The outcome of the case remains uncertain.\n",
      "\n",
      " Resumen mt5:\n",
      "In the last episode of Yep, the CEO of Blizzard, South Park, has told the BBC he is going to leave the US.\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 6\n",
      "Título: My new wooden monstera sculpture.\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "El hilo de Reddit describe un nuevo trabajo de arte en madera de madera. El usuario está satisfecho con el trabajo y lo describe como \"phenomenal\". El hilo está lleno de elogios y comentarios positivos.\n",
      "\n",
      " Resumen mt5:\n",
      "A new wooden monstera sculpture has been unveiled by the British National Lottery. Here's a selection of images from the artist and craftsman.\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 7\n",
      "Título: $1.4 trillion was erased from the stock market today.\n",
      "Descripción: $1.4 trillion was erased from the stock market today.\n",
      "\n",
      "And DJT says: “There can be a slowing of the economy unless rates are cut.”\n",
      "\n",
      "—•—\n",
      "\n",
      "Data for the last 20 years showed that 7/10 best days occurred within 2 weeks of the 10 worst days.\n",
      "\n",
      "—•—\n",
      "\n",
      "What do you think?\n",
      "\n",
      "Resumen SLM:\n",
      "El hilo de Reddit está sobre el impacto del mercado de valores en los últimos 20 años. Se menciona que el mercado ha perdido $1.4 trillion en los últimos días, lo que ha generado especulaciones sobre la posibilidad de un slowing del crecimiento económico. Se proporciona un análisis de los datos y se sugiere que el mercado está sobrevalorado.\n",
      "\n",
      " Resumen mt5:\n",
      "US President Donald Trump says the US economy is going to be hit by a slowing of interest rates. Here is his reaction to his tweet.\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 8\n",
      "Título: \"We Are Fighting Against a Dictator Backed by a Traitor\" – A French Senator Speaks Out\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "Este discurso es un ejemplo de la claridad y la honestidad de un político. Es un discurso que nos recuerda la importancia de la libertad y la justicia, y nos desafía a ser responsables de nuestros propios gobiernos.\n",
      "\n",
      " Resumen mt5:\n",
      "This is a full transcript of Donald Trump's speech to the US congress on the Ukrainian crisis. Here is the full text.\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 9\n",
      "Título: What do you think about it?\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "El hilo está sobre la interpretación de memes y la naturaleza de la economía. Se menciona que el mercado está racional y que los principios financieros no siempre funcionan como se creen. Se destaca la importancia de la gestión del riesgo y la búsqueda de la riqueza a largo plazo.\n",
      "\n",
      " Resumen mt5:\n",
      "It's a year when millions of people across the world are celebrating the start of 2021. But what do you think about it?\n",
      "====================================================================================================\n",
      "\n",
      "Hilo 10\n",
      "Título: PCMR HAS HIT 4 MILLION SUBS! TIME TO CELEBRATE WITH A GIVEAWAY!\n",
      "Descripción: \n",
      "\n",
      "Resumen SLM:\n",
      "PCMR ha alcanzado 4 millones de suscriptores. Para entrar en el concurso, comentarios en este hilo y regístrate para participar. Hay tres ganadores de suerte:\n",
      "\n",
      "* /u/shadowcoll\n",
      "* /u/MrSneakMan_\n",
      "* /u/GlassOfOrange247\n",
      "\n",
      "El concurso ha terminado. El ganador ha sido elegido y notificado.\n",
      "\n",
      " Resumen mt5:\n",
      "The BBC’s weekly The Boss series profiles different readers from around the world. This week we speak to the winners of the PCmasterrace.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Ruta del archivo con todos los hilos\n",
    "ruta_json = \"10_hilos_para_evaluacion.json\"\n",
    "\n",
    "# Cargar hilos desde el JSON\n",
    "with open(ruta_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    hilos = json.load(f)\n",
    "\n",
    "# Mostrar solo el resumen de cada hilo (más título y descripción)\n",
    "for i, hilo in enumerate(hilos, 1):\n",
    "    print(f\"\\nHilo {i}\")\n",
    "    print(f\"Título: {hilo.get('title', '').strip()}\")\n",
    "    print(f\"Descripción: {hilo.get('description', '').strip()}\\n\")\n",
    "\n",
    "    # Mostrar resumen del modelo SLM\n",
    "    print(\"Resumen SLM:\")\n",
    "    print(hilo.get(\"summary_slm\", \"\").strip() or \"(no disponible)\")\n",
    "\n",
    "    # Mostrar resumen de otro enfoque (si existe)\n",
    "    if \"summary_mt5\" in hilo:\n",
    "        print(\"\\n Resumen mt5:\")\n",
    "        print(hilo.get(\"summary_mt5\", \"\").strip() or \"(no disponible)\")\n",
    "\n",
    "    print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Detección de contenido inapropiado usando ZSL, FSL y Chain-of-thought**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aqui se cargan los hilos del subredit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos de r/OpinionesPolemicas...\n",
      "✅ Descarga completa.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "import praw\n",
    "\n",
    "# --- Conexión a Reddit ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id='id',\n",
    "    client_secret='passwd',\n",
    "    user_agent='user'\n",
    ")\n",
    "\n",
    "# --- Subreddit específico ---\n",
    "subreddit_name = 'OpinionesPolemicas'\n",
    "\n",
    "# --- Carpeta para guardar JSONs ---\n",
    "output_folder = 'OpinionesPolemicas_jsons'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Función para convertir timestamp ---\n",
    "def convert_date(utc_timestamp):\n",
    "    return dt.datetime.fromtimestamp(utc_timestamp).isoformat()\n",
    "\n",
    "# --- Extracción de datos ---\n",
    "print(f\"Descargando datos de r/{subreddit_name}...\")\n",
    "\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "threads_data = []\n",
    "\n",
    "for i, submission in enumerate(subreddit.top(limit=10)):  # 10 hilos\n",
    "    if i >= 10:\n",
    "        break\n",
    "    try:\n",
    "        submission.comments.replace_more(limit=0)  # Descargar todos los comentarios directamente\n",
    "    except Exception as e:\n",
    "        print(f\"Error en los comentarios de {submission.title}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    thread_info = {\n",
    "        \"flair\": submission.link_flair_text if submission.link_flair_text else \"\",\n",
    "        \"title\": submission.title,\n",
    "        \"author\": str(submission.author) if submission.author else \"Deleted\",\n",
    "        \"date\": convert_date(submission.created_utc),\n",
    "        \"score\": submission.score,\n",
    "        \"description\": submission.selftext,\n",
    "        \"comments\": []\n",
    "    }\n",
    "    \n",
    "    comments_count = 0\n",
    "    for comment in submission.comments.list():\n",
    "        if comment.body and len(comment.body) > 10:  # Filtrar comentarios vacíos o muy cortos\n",
    "            comment_info = {\n",
    "                \"user\": str(comment.author) if comment.author else \"Deleted\",\n",
    "                \"comment\": comment.body,\n",
    "                \"score\": comment.score,\n",
    "                \"date\": convert_date(comment.created_utc)\n",
    "            }\n",
    "            thread_info[\"comments\"].append(comment_info)\n",
    "            comments_count += 1\n",
    "            if comments_count >= 50:\n",
    "                break\n",
    "    \n",
    "    if len(thread_info[\"comments\"]) > 0:\n",
    "        threads_data.append(thread_info)\n",
    "\n",
    "# --- Guardar en JSON ---\n",
    "output_path = os.path.join(output_folder, f\"{subreddit_name}.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(threads_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Descarga completa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e2035b7fbf4d6f80b91f1e463c7311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cpu\n",
      "c:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m texto \u001b[38;5;241m=\u001b[39m comment\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m texto\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m---> 53\u001b[0m     comment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minapropiado_zsl\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m clasificar_comentario(texto)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     comment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minapropiado_zsl\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvacío\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[35], line 37\u001b[0m, in \u001b[0;36mclasificar_comentario\u001b[1;34m(texto)\u001b[0m\n\u001b[0;32m     35\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m¿Este comentario contiene contenido inapropiado?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mComentario: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtexto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRespuesta:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m     respuesta \u001b[38;5;241m=\u001b[39m classifier(prompt)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msí\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m respuesta \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msi\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m respuesta:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msí\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:287\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1371\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1364\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1365\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1368\u001b[0m         )\n\u001b[0;32m   1369\u001b[0m     )\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1378\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1377\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1378\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1379\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1278\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1277\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1278\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1279\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:385\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    383\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 385\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[0;32m    388\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2323\u001b[0m     )\n\u001b[0;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2327\u001b[0m         input_ids,\n\u001b[0;32m   2328\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2329\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2330\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2331\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2332\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2333\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2334\u001b[0m     )\n\u001b[0;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2343\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:3289\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3287\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3289\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3291\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3292\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3293\u001b[0m     outputs,\n\u001b[0;32m   3294\u001b[0m     model_kwargs,\n\u001b[0;32m   3295\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3296\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:855\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    856\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    857\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    858\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    859\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    860\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    861\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    862\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    863\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    864\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    865\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    869\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:604\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    593\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    594\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    601\u001b[0m         position_embeddings,\n\u001b[0;32m    602\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 604\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m    606\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    607\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    608\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    609\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    610\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    611\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    612\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    613\u001b[0m     )\n\u001b[0;32m    615\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:355\u001b[0m, in \u001b[0;36mGemmaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    354\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 355\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    356\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    358\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:98\u001b[0m, in \u001b[0;36mGemmaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 98\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\accelerate\\hooks.py:171\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 171\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\accelerate\\hooks.py:342\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(\n\u001b[0;32m    336\u001b[0m     module,\n\u001b[0;32m    337\u001b[0m     include_buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_buffers,\n\u001b[0;32m    338\u001b[0m     recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_submodules,\n\u001b[0;32m    339\u001b[0m     remove_non_persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    340\u001b[0m ):\n\u001b[0;32m    341\u001b[0m     fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name]\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\accelerate\\utils\\offload.py:118\u001b[0m, in \u001b[0;36mPrefixedDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Escritorioanaconda3\\Lib\\site-packages\\accelerate\\utils\\offload.py:180\u001b[0m, in \u001b[0;36mOffloadedWeightsLoader.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m weight_info:\n\u001b[0;32m    178\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mgetattr\u001b[39m(torch, weight_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m    181\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- Login (asegúrate de tener el token correcto) ---\n",
    "login(\"token\")  # <-- Reemplaza por tu token Hugging Face real\n",
    "\n",
    "# --- Modelo Gemma 2B ---\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# --- Cargar JSON original ---\n",
    "with open(\"OpinionesPolemicas_jsons/OpinionesPolemicas.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    threads = json.load(f)\n",
    "\n",
    "# --- Clasificación ZSL de comentarios ---\n",
    "def clasificar_comentario(texto):\n",
    "    prompt = f\"¿Este comentario contiene contenido inapropiado?\\n\\nComentario: \\\"{texto}\\\"\\nRespuesta:\"\n",
    "    try:\n",
    "        respuesta = classifier(prompt)[0][\"generated_text\"].lower()\n",
    "        if \"sí\" in respuesta or \"si\" in respuesta:\n",
    "            return \"sí\"\n",
    "        elif \"no\" in respuesta:\n",
    "            return \"no\"\n",
    "        else:\n",
    "            return \"indeterminado\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error en comentario: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "# --- Procesar todos los hilos y comentarios ---\n",
    "for thread in threads:\n",
    "    for comment in thread[\"comments\"]:\n",
    "        texto = comment.get(\"comment\", \"\")\n",
    "        if texto.strip():\n",
    "            comment[\"inapropiado_zsl\"] = clasificar_comentario(texto)\n",
    "        else:\n",
    "            comment[\"inapropiado_zsl\"] = \"vacío\"\n",
    "\n",
    "# --- Guardar el resultado ---\n",
    "with open(\"OpinionesPolemicas_jsons/OpinionesPolemicas_evaluado.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(threads, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ Comentarios clasificados con ZSL.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En este ejercicio, hemos abordado la detección de contenido sensible en hilos del subreddit OpinionesPolemicas utilizando un enfoque de Zero-shot Learning apoyado en un modelo SLM, específicamente google/gemma-3-1b-it, accedido mediante la API de OpenRouter.**\n",
    "\n",
    "**Hemos diseñado un sistema que formula preguntas directas al modelo sobre la sensibilidad de cada comentario, recogiendo respuestas categóricas (YES/NO) que luego se traducen a etiquetas binarias (True/False) en un nuevo campo \"sensitive\" dentro del archivo JSON.**\n",
    "\n",
    "**A pesar de que el planteamiento que hemos pensado ejecuta correctamente la clasificación, los resultados observados en los archivos finales revelan una clara limitación del modelo en el contexto en español, ya que todos los comentarios, incluso algunos que contienen lenguaje ofensivo o connotaciones polémicas como “saludo nazi” o “mierdas”, han sido marcados como false (no sensibles), y sin razonamiento añadido (sensitive_reasoning = \"\").**\n",
    "\n",
    "**Esto sugiere que nuestro modelo o el prompt actual no están interpretando adecuadamente el contenido o no están ajustados para el idioma y dominio, lo cual resalta la importancia de afinar los prompts, usar modelos adaptados al español, o aplicar estrategias más robustas como chain-of-thought o few-shot para mejorar la sensibilidad y precisión del sistema.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
